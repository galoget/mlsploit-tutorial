# Adversarial Machine Learning with MLsploit 


## Course Summary

Machine learning (ML) has been used in many real-world applications. However, recent research has shown that ML models can be highly vulnerable to adversarial examples, which are input instances that are intentionally designed to fool a model into producing a chosen prediction. The goal of this course is to demonstrate ML vulnerabilities and develop secure AI in various domains, with MLsploit.


MLsploit is an ML evaluation and fortification framework designed for education and research. It focuses on ML security related techniques in adversarial settings, such as adversarial creation, detection, and countermeasure. It consists of plug-able components or services which could demonstrate various security-related research topics.


Several built-in modules in MLsploit will be shown in this course, including defense in the image domain (SHIELD), malware detection and bypassing (AVPass, ELF, Barnum), and the application of Intel SGX for privacy-preserving and inference-preventing ML.


To get started, head over to the [MLsploit REST API](https://github.com/mlsploit/mlsploit-rest-api), [MLsploit Execution Backend](https://github.com/mlsploit/mlsploit-execution-backend) and [MLsploit Web UI](https://github.com/mlsploit/mlsploit-web-ui) repositories to set up MLsploit, and then check out the various MLsploit modules on the [course homepage](https://mlsploit.github.io/mlsploit-tutorial/).
